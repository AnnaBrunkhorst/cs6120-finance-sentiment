{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:06:39.002323Z",
     "start_time": "2024-08-15T06:06:38.999558Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from utils.DataUtils import DataUtils\n",
    "from utils.GloveUtils import GloveUtils"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load split data\n",
    "yahoo_train = pd.read_csv(\"./data/yahoo_train.csv\")\n",
    "yahoo_test = pd.read_csv(\"./data/yahoo_test.csv\")\n",
    "\n",
    "# Initialize custom util library\n",
    "utils = DataUtils()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:10:52.360937Z",
     "start_time": "2024-08-15T06:10:52.322831Z"
    }
   },
   "id": "30d4d0298a530be6",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(yahoo_train['label'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:06:52.978631Z",
     "start_time": "2024-08-15T06:06:52.898660Z"
    }
   },
   "id": "290da01ff6310ff3",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parse, tokenize, and lemmatize"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cb82c65eab0cd99"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "yahoo_train['clean_txt'] = utils.clean_data(yahoo_train['text'], max_words=40)\n",
    "yahoo_test['clean_txt'] = utils.clean_data(yahoo_test['text'], max_words=40)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:14:04.389473Z",
     "start_time": "2024-08-15T06:11:44.857943Z"
    }
   },
   "id": "184539556dc18399",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Embedding shape: (400000, 100)\n"
     ]
    }
   ],
   "source": [
    "GLOVE_PATH = \"./data/glove.6B.100d.txt\"\n",
    "glove_utils = GloveUtils(GLOVE_PATH, max_dims=64)\n",
    "\n",
    "glove_emb_layer = glove_utils.create_glove_emb_layer(trainable=True)\n",
    "train_emb_ids = glove_utils.get_embedding_indices(yahoo_train['clean_txt'])\n",
    "test_emb_ids = glove_utils.get_embedding_indices(yahoo_test['clean_txt'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:14:58.844194Z",
     "start_time": "2024-08-15T06:14:50.775324Z"
    }
   },
   "id": "1b53ec82058fddec",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True\n",
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f'CUDA: {torch.cuda.is_available()}')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'DEVICE: {device}')\n",
    "torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:14:58.849520Z",
     "start_time": "2024-08-15T06:14:58.845224Z"
    }
   },
   "id": "c2459029e85ebee2",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_emb_ids = train_emb_ids.to(device)\n",
    "test_emb_ids = test_emb_ids.to(device)\n",
    "\n",
    "y_train = torch.Tensor(np.asarray(yahoo_train['label'], dtype=np.uint8)).to(device)\n",
    "y_test = torch.Tensor(np.asarray(yahoo_test['label'], dtype=np.uint8)).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:15:04.860371Z",
     "start_time": "2024-08-15T06:15:04.285622Z"
    }
   },
   "id": "bca7452f48c42bd7",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: torch.Size([17522, 64])\n",
      "y: torch.Size([17522])\n"
     ]
    }
   ],
   "source": [
    "print(f'X: {train_emb_ids.shape}')\n",
    "print(f'y: {y_train.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:15:07.262505Z",
     "start_time": "2024-08-15T06:15:07.259045Z"
    }
   },
   "id": "30f3d4f1f3584ad",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37492539 0.25406162 0.371013  ]\n"
     ]
    }
   ],
   "source": [
    "# Finally Embedding layer to the GPU\n",
    "glove_emb_layer = glove_emb_layer.to(device)\n",
    "\n",
    "_, cts = np.unique(torch.Tensor.cpu(y_train), return_counts=True)\n",
    "class_weights = 1.0 / cts\n",
    "class_weights /= class_weights.sum()\n",
    "print(class_weights)\n",
    "class_weights = torch.Tensor(class_weights).to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:15:10.302366Z",
     "start_time": "2024-08-15T06:15:10.270207Z"
    }
   },
   "id": "92d351418673fff8",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8192\n",
    "NUM_LABELS = 3\n",
    "MAX_WORDS = 40\n",
    "\n",
    "def setup_model(num_classes=NUM_LABELS, emb_layer=glove_emb_layer):\n",
    "    return nn.Sequential(\n",
    "        emb_layer,\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(64 * 100, 2048),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(2048, 512),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(512, 64),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Linear(64, num_classes),\n",
    "        nn.Softmax(dim=1)\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:15:14.292574Z",
     "start_time": "2024-08-15T06:15:14.289253Z"
    }
   },
   "id": "9a8604fe8c9119b9",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_model(model, X_train, X_test, y_train, y_test, n_epochs=50, lr=0.001):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=100, gamma=0.1)\n",
    "    batch_per_epoch = len(X_train) // BATCH_SIZE\n",
    "\n",
    "    tr_loss_hist = np.zeros((n_epochs,), dtype=np.float32)\n",
    "    ts_loss_hist = np.zeros((n_epochs,), dtype=np.float32)\n",
    "    ts_acc_hist = np.zeros((n_epochs), dtype=np.float32)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        print(f'*** EPOCH {epoch} ***')\n",
    "        print(f'lr: {scheduler.get_last_lr()[0]}')\n",
    "        tr_loss = 0.\n",
    "        model.train(mode=True)\n",
    "        for i in range(batch_per_epoch):\n",
    "            st = i * BATCH_SIZE\n",
    "            X_batch = X_train[st:st + BATCH_SIZE]\n",
    "            y_batch = y_train[st:st + BATCH_SIZE]\n",
    "            # Reset gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # predict / forward pass\n",
    "            y_logits = model(X_batch)\n",
    "            loss = loss_fn(y_logits, y_batch.to(torch.uint8))\n",
    "            tr_loss += loss\n",
    "            # tune / backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        tr_loss /= batch_per_epoch\n",
    "        optimizer.zero_grad()\n",
    "        # test\n",
    "        with torch.no_grad():\n",
    "            y_pred_logits = model(X_test)\n",
    "            acc = accuracy_score(torch.Tensor.cpu(y_test), np.argmax(torch.Tensor.cpu(y_pred_logits), axis=1))\n",
    "            test_loss = loss_fn(y_pred_logits, y_test.to(torch.uint8))\n",
    "        \n",
    "        print(\"Train Loss {:.2f}\".format(tr_loss))\n",
    "        print(\"Test Loss {:.2f}\".format(test_loss))\n",
    "        print(\"Test Accuracy {:.2f} \\n\".format(acc))\n",
    "        tr_loss_hist[epoch] = tr_loss\n",
    "        ts_loss_hist[epoch] = test_loss\n",
    "        ts_acc_hist[epoch] = acc\n",
    "    return tr_loss_hist, ts_loss_hist, ts_acc_hist"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:15:19.949866Z",
     "start_time": "2024-08-15T06:15:19.932997Z"
    }
   },
   "id": "364b78fd8661150e",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Embedding(400000, 100)\n",
      "  (1): Flatten(start_dim=1, end_dim=-1)\n",
      "  (2): Linear(in_features=6400, out_features=2048, bias=True)\n",
      "  (3): Dropout(p=0.3, inplace=False)\n",
      "  (4): LeakyReLU(negative_slope=0.01)\n",
      "  (5): Linear(in_features=2048, out_features=512, bias=True)\n",
      "  (6): Dropout(p=0.3, inplace=False)\n",
      "  (7): LeakyReLU(negative_slope=0.01)\n",
      "  (8): Linear(in_features=512, out_features=64, bias=True)\n",
      "  (9): LeakyReLU(negative_slope=0.01)\n",
      "  (10): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (11): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = setup_model(num_classes=NUM_LABELS)\n",
    "model = model.to(device)\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:15:23.392065Z",
     "start_time": "2024-08-15T06:15:23.297088Z"
    }
   },
   "id": "28c88cfe2c5c8562",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** EPOCH 0 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.11\n",
      "Test Loss 1.09\n",
      "Test Accuracy 0.41 \n",
      "\n",
      "*** EPOCH 1 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.08\n",
      "Test Loss 1.07\n",
      "Test Accuracy 0.42 \n",
      "\n",
      "*** EPOCH 2 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.07\n",
      "Test Loss 1.06\n",
      "Test Accuracy 0.44 \n",
      "\n",
      "*** EPOCH 3 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.06\n",
      "Test Loss 1.06\n",
      "Test Accuracy 0.44 \n",
      "\n",
      "*** EPOCH 4 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.05\n",
      "Test Loss 1.05\n",
      "Test Accuracy 0.45 \n",
      "\n",
      "*** EPOCH 5 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.05\n",
      "Test Loss 1.04\n",
      "Test Accuracy 0.47 \n",
      "\n",
      "*** EPOCH 6 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.03\n",
      "Test Loss 1.03\n",
      "Test Accuracy 0.49 \n",
      "\n",
      "*** EPOCH 7 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.01\n",
      "Test Loss 1.02\n",
      "Test Accuracy 0.49 \n",
      "\n",
      "*** EPOCH 8 ***\n",
      "lr: 0.001\n",
      "Train Loss 1.00\n",
      "Test Loss 0.99\n",
      "Test Accuracy 0.52 \n",
      "\n",
      "*** EPOCH 9 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.98\n",
      "Test Loss 0.99\n",
      "Test Accuracy 0.52 \n",
      "\n",
      "*** EPOCH 10 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.98\n",
      "Test Loss 0.97\n",
      "Test Accuracy 0.54 \n",
      "\n",
      "*** EPOCH 11 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.96\n",
      "Test Loss 0.97\n",
      "Test Accuracy 0.54 \n",
      "\n",
      "*** EPOCH 12 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.96\n",
      "Test Loss 0.97\n",
      "Test Accuracy 0.54 \n",
      "\n",
      "*** EPOCH 13 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.94\n",
      "Test Loss 0.96\n",
      "Test Accuracy 0.55 \n",
      "\n",
      "*** EPOCH 14 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.94\n",
      "Test Loss 0.96\n",
      "Test Accuracy 0.55 \n",
      "\n",
      "*** EPOCH 15 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.93\n",
      "Test Loss 0.94\n",
      "Test Accuracy 0.57 \n",
      "\n",
      "*** EPOCH 16 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.92\n",
      "Test Loss 0.94\n",
      "Test Accuracy 0.58 \n",
      "\n",
      "*** EPOCH 17 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.91\n",
      "Test Loss 0.93\n",
      "Test Accuracy 0.58 \n",
      "\n",
      "*** EPOCH 18 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.90\n",
      "Test Loss 0.93\n",
      "Test Accuracy 0.59 \n",
      "\n",
      "*** EPOCH 19 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.89\n",
      "Test Loss 0.92\n",
      "Test Accuracy 0.60 \n",
      "\n",
      "*** EPOCH 20 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.88\n",
      "Test Loss 0.92\n",
      "Test Accuracy 0.60 \n",
      "\n",
      "*** EPOCH 21 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.88\n",
      "Test Loss 0.93\n",
      "Test Accuracy 0.60 \n",
      "\n",
      "*** EPOCH 22 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.87\n",
      "Test Loss 0.93\n",
      "Test Accuracy 0.59 \n",
      "\n",
      "*** EPOCH 23 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.87\n",
      "Test Loss 0.93\n",
      "Test Accuracy 0.60 \n",
      "\n",
      "*** EPOCH 24 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.87\n",
      "Test Loss 0.92\n",
      "Test Accuracy 0.61 \n",
      "\n",
      "*** EPOCH 25 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.85\n",
      "Test Loss 0.92\n",
      "Test Accuracy 0.61 \n",
      "\n",
      "*** EPOCH 26 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.85\n",
      "Test Loss 0.93\n",
      "Test Accuracy 0.60 \n",
      "\n",
      "*** EPOCH 27 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.86\n",
      "Test Loss 0.91\n",
      "Test Accuracy 0.62 \n",
      "\n",
      "*** EPOCH 28 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.84\n",
      "Test Loss 0.92\n",
      "Test Accuracy 0.60 \n",
      "\n",
      "*** EPOCH 29 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.84\n",
      "Test Loss 0.91\n",
      "Test Accuracy 0.62 \n",
      "\n",
      "*** EPOCH 30 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.83\n",
      "Test Loss 0.90\n",
      "Test Accuracy 0.63 \n",
      "\n",
      "*** EPOCH 31 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.83\n",
      "Test Loss 0.89\n",
      "Test Accuracy 0.65 \n",
      "\n",
      "*** EPOCH 32 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.80\n",
      "Test Loss 0.89\n",
      "Test Accuracy 0.65 \n",
      "\n",
      "*** EPOCH 33 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.79\n",
      "Test Loss 0.89\n",
      "Test Accuracy 0.64 \n",
      "\n",
      "*** EPOCH 34 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.78\n",
      "Test Loss 0.89\n",
      "Test Accuracy 0.65 \n",
      "\n",
      "*** EPOCH 35 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.78\n",
      "Test Loss 0.88\n",
      "Test Accuracy 0.65 \n",
      "\n",
      "*** EPOCH 36 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.77\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.66 \n",
      "\n",
      "*** EPOCH 37 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.76\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.67 \n",
      "\n",
      "*** EPOCH 38 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.75\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.67 \n",
      "\n",
      "*** EPOCH 39 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.75\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.66 \n",
      "\n",
      "*** EPOCH 40 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.75\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 41 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.73\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 42 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.72\n",
      "Test Loss 0.88\n",
      "Test Accuracy 0.66 \n",
      "\n",
      "*** EPOCH 43 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.80\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.66 \n",
      "\n",
      "*** EPOCH 44 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.80\n",
      "Test Loss 0.88\n",
      "Test Accuracy 0.66 \n",
      "\n",
      "*** EPOCH 45 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.77\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.67 \n",
      "\n",
      "*** EPOCH 46 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.75\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.67 \n",
      "\n",
      "*** EPOCH 47 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.74\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 48 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.73\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.67 \n",
      "\n",
      "*** EPOCH 49 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.73\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 50 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.72\n",
      "Test Loss 0.88\n",
      "Test Accuracy 0.66 \n",
      "\n",
      "*** EPOCH 51 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.72\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 52 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.71\n",
      "Test Loss 0.87\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 53 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.72\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.67 \n",
      "\n",
      "*** EPOCH 54 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.71\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 55 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.70\n",
      "Test Loss 0.85\n",
      "Test Accuracy 0.69 \n",
      "\n",
      "*** EPOCH 56 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.69\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.71 \n",
      "\n",
      "*** EPOCH 57 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.68\n",
      "Test Loss 0.84\n",
      "Test Accuracy 0.70 \n",
      "\n",
      "*** EPOCH 58 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.67\n",
      "Test Loss 0.85\n",
      "Test Accuracy 0.69 \n",
      "\n",
      "*** EPOCH 59 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.67\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.71 \n",
      "\n",
      "*** EPOCH 60 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.67\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.71 \n",
      "\n",
      "*** EPOCH 61 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.66\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.71 \n",
      "\n",
      "*** EPOCH 62 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.66\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 63 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.66\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 64 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.65\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 65 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.65\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 66 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.65\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 67 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.64\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 68 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.64\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 69 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.64\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 70 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 71 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 72 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 73 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 74 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 75 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 76 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 77 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 78 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 79 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 80 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 81 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 82 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 83 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 84 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 85 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 86 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 87 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 88 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 89 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 90 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.72 \n",
      "\n",
      "*** EPOCH 91 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 92 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.85\n",
      "Test Accuracy 0.69 \n",
      "\n",
      "*** EPOCH 93 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.65\n",
      "Test Loss 0.86\n",
      "Test Accuracy 0.68 \n",
      "\n",
      "*** EPOCH 94 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.67\n",
      "Test Loss 0.83\n",
      "Test Accuracy 0.71 \n",
      "\n",
      "*** EPOCH 95 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.65\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 96 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.63\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 97 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 98 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.82\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 99 ***\n",
      "lr: 0.001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 100 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 101 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.62\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 102 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 103 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 104 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 105 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 106 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 107 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 108 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 109 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 110 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 111 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 112 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 113 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 114 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 115 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 116 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 117 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 118 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 119 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 120 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 121 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 122 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 123 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 124 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 125 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 126 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 127 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 128 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 129 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 130 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 131 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 132 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 133 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 134 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 135 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 136 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 137 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 138 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 139 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 140 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 141 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 142 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 143 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 144 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 145 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 146 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 147 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 148 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 149 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 150 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 151 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 152 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 153 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 154 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 155 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 156 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 157 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 158 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 159 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 160 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 161 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 162 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 163 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 164 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 165 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 166 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 167 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 168 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 169 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 170 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 171 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 172 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 173 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 174 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 175 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 176 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 177 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 178 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 179 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 180 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 181 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 182 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 183 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 184 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 185 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 186 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 187 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 188 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 189 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 190 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 191 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 192 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 193 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 194 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 195 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 196 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 197 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 198 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 199 ***\n",
      "lr: 0.0001\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 200 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 201 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 202 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 203 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 204 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 205 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 206 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 207 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 208 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 209 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 210 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 211 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 212 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 213 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 214 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 215 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 216 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 217 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 218 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 219 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 220 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 221 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 222 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 223 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 224 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 225 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 226 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 227 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 228 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 229 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 230 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 231 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 232 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 233 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 234 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 235 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 236 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 237 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 238 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 239 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 240 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 241 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 242 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 243 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 244 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 245 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 246 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 247 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 248 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 249 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 250 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 251 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 252 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 253 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 254 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 255 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 256 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 257 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 258 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 259 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 260 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 261 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 262 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 263 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 264 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 265 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 266 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 267 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 268 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 269 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 270 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 271 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 272 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 273 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 274 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 275 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 276 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 277 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 278 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 279 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.73 \n",
      "\n",
      "*** EPOCH 280 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 281 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 282 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 283 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 284 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 285 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 286 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 287 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 288 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 289 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 290 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 291 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 292 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 293 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 294 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 295 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 296 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 297 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 298 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n",
      "\n",
      "*** EPOCH 299 ***\n",
      "lr: 1e-05\n",
      "Train Loss 0.61\n",
      "Test Loss 0.81\n",
      "Test Accuracy 0.74 \n"
     ]
    }
   ],
   "source": [
    "tr_loss_hist, ts_loss_hist, ts_acc_hist = train_model(model, train_emb_ids, test_emb_ids, y_train, y_test, n_epochs=300)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:16:22.544600Z",
     "start_time": "2024-08-15T06:15:27.809439Z"
    }
   },
   "id": "1c6544d2eb0208a8",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "MODEL_NAME = \"yahoo_2048_final_ep_300\"\n",
    "torch.save(model, f\"./models/{MODEL_NAME}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:16:27.891668Z",
     "start_time": "2024-08-15T06:16:27.654130Z"
    }
   },
   "id": "2ff568445d749572",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "training_hist = pd.DataFrame.from_dict({'train_loss': tr_loss_hist, 'test_loss': ts_loss_hist, 'accuracy': ts_acc_hist})\n",
    "training_hist.to_csv(f'./models/{MODEL_NAME + '_training_history'}.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:16:30.461411Z",
     "start_time": "2024-08-15T06:16:30.454324Z"
    }
   },
   "id": "c3a9ac3db84e1bff",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.67      0.66      0.66      2204\n",
      "         1.0       0.81      0.81      0.81      3167\n",
      "         2.0       0.68      0.70      0.69      2139\n",
      "\n",
      "    accuracy                           0.73      7510\n",
      "   macro avg       0.72      0.72      0.72      7510\n",
      "weighted avg       0.73      0.73      0.73      7510\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = np.argmax(torch.Tensor.cpu(model(test_emb_ids)), axis=1)\n",
    "    print(classification_report(torch.Tensor.cpu(y_test), y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-15T06:16:44.779390Z",
     "start_time": "2024-08-15T06:16:44.506408Z"
    }
   },
   "id": "2304f8759b0a4bec",
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
